{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network demo on the Titanic dataset\n",
    "#### Using the neural network that I've built, I am going to train it on the Titanic dataset, which contains data about a subset of people on Titanic, including if they survived or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from NeuralNetwork.Preprocessing.OneHotEncoder import oneHotEncode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from NeuralNetwork.NeuralNetwork import NeuralNetwork\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uploading the dataset and preprocessing it.\n",
    "- Splitting the dataset into x and y.\n",
    "- Dropping irelevant columns.\n",
    "- One hot encoding the categorical features.\n",
    "- Applying mean normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Data\\\\titanic.csv\")                     #load the dataset\n",
    "\n",
    "y = dataset['Survived']                                        #split into dependent\n",
    "x = dataset.drop(columns=['Survived'])                         #          and independent variables\n",
    "\n",
    "x = x.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin']) #drop the redundant features\n",
    "x = pd.get_dummies(x, columns=['Pclass', 'Sex', 'Embarked'])   #one hot encode the categorial features\n",
    "\n",
    "x['Age'].fillna(x['Age'].mean(), inplace=True)                 #fill missing values in Age and Fare  \n",
    "x['Fare'].fillna(x['Fare'].mean(), inplace=True)               # with their mean value, so the distribution doesn't change\n",
    "\n",
    "x=(x-x.mean())/x.std()                                         #mean normalization\n",
    "\n",
    "x, y = x.values, y.values                                      #convert to numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the data into training and test sets. (80% for training, 20% for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, xts, ytr, yts = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network parameters:\n",
    "- Fully connected network, 12 neurons as input, two hidden layers with 32 and 4 neurons and one neuron for output.\n",
    "- Binary crossentropy (log loss) as cost function.\n",
    "- Sigmoid as activation function.\n",
    "- Batch gradient descend for backpropagation.\n",
    "- Batch size: 16\n",
    "- Learning rate:0.4 \n",
    "- Lambda (regularization parameter):0.01\n",
    "- Dropout probability:0.1\n",
    "\n",
    "##### Creating the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([12,16,8,1],'sigmoid','cross-entropy',weightsRange=[-0.1,0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the network with the parameters specified above.\n",
    "- Before training, the network splits the data in 80% training and 20% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 569 samples, validating on 143.\n",
      "Epoch 0. Loss train 0.6711645051822093; validation 0.6363916549323816. \n",
      "Epoch 1. Loss train 0.6713548999335256; validation 0.6336348539814969. \n",
      "Epoch 2. Loss train 0.6713588638900586; validation 0.6334061122135178. \n",
      "Epoch 3. Loss train 0.6713340680850239; validation 0.633366898826175. \n",
      "Epoch 4. Loss train 0.6713065718491859; validation 0.6333429238913434. \n",
      "Epoch 5. Loss train 0.6712784095905425; validation 0.6333199079159532. \n",
      "Epoch 6. Loss train 0.6712495668534363; validation 0.633296529669139. \n",
      "Epoch 7. Loss train 0.6712198543306914; validation 0.6332725243655083. \n",
      "Epoch 8. Loss train 0.6711890591149211; validation 0.6332477074055889. \n",
      "Epoch 9. Loss train 0.6711569543500588; validation 0.6332218920396351. \n",
      "Epoch 10. Loss train 0.6711232969965447; validation 0.6331948803140689. \n",
      "Epoch 11. Loss train 0.6710878242762885; validation 0.6331664598242279. \n",
      "Epoch 12. Loss train 0.671050249579541; validation 0.6331364006472252. \n",
      "Epoch 13. Loss train 0.6710102577957516; validation 0.6331044519171525. \n",
      "Epoch 14. Loss train 0.670967499943738; validation 0.6330703379128232. \n",
      "Epoch 15. Loss train 0.6709215869452996; validation 0.6330337535456244. \n",
      "Epoch 16. Loss train 0.6708720823536332; validation 0.632994359115742. \n",
      "Epoch 17. Loss train 0.6708184938072856; validation 0.632951774177343. \n",
      "Epoch 18. Loss train 0.6707602629293935; validation 0.63290557031825. \n",
      "Epoch 19. Loss train 0.6706967533278778; validation 0.6328552626155076. \n",
      "Epoch 20. Loss train 0.6706272362715355; validation 0.6328002994726429. \n",
      "Epoch 21. Loss train 0.670550873515085; validation 0.6327400504741553. \n",
      "Epoch 22. Loss train 0.6704666966173334; validation 0.6326737918037592. \n",
      "Epoch 23. Loss train 0.6703735819331541; validation 0.6326006886597973. \n",
      "Epoch 24. Loss train 0.6702702202521301; validation 0.6325197739571015. \n",
      "Epoch 25. Loss train 0.6701550797918694; validation 0.632429922420315. \n",
      "Epoch 26. Loss train 0.6700263609157563; validation 0.6323298189375048. \n",
      "Epoch 27. Loss train 0.6698819405121628; validation 0.6322179197392752. \n",
      "Epoch 28. Loss train 0.6697193034176622; validation 0.6320924045774214. \n",
      "Epoch 29. Loss train 0.6695354575557019; validation 0.6319511175720846. \n",
      "Epoch 30. Loss train 0.6693268285500206; validation 0.6317914937432748. \n",
      "Epoch 31. Loss train 0.6690891284027173; validation 0.6316104673973667. \n",
      "Epoch 32. Loss train 0.668817191330371; validation 0.631404357445165. \n",
      "Epoch 33. Loss train 0.6685047679424905; validation 0.6311687233138001. \n",
      "Epoch 34. Loss train 0.6681442665233542; validation 0.6308981832909132. \n",
      "Epoch 35. Loss train 0.6677264271256326; validation 0.6305861847986806. \n",
      "Epoch 36. Loss train 0.6672399103806296; validation 0.6302247131126197. \n",
      "Epoch 37. Loss train 0.6666707782660158; validation 0.6298039212830205. \n",
      "Epoch 38. Loss train 0.6660018384866796; validation 0.6293116593660593. \n",
      "Epoch 39. Loss train 0.6652118176764918; validation 0.6287328754700876. \n",
      "Epoch 40. Loss train 0.6642743216293435; validation 0.6280488546745142. \n",
      "Epoch 41. Loss train 0.6631565340285057; validation 0.6272362550465989. \n",
      "Epoch 42. Loss train 0.6618176004454736; validation 0.6262658939699826. \n",
      "Epoch 43. Loss train 0.6602066453715519; validation 0.6251012354970887. \n",
      "Epoch 44. Loss train 0.6582603839762765; validation 0.6236965360405975. \n",
      "Epoch 45. Loss train 0.6559003313215036; validation 0.6219946325946315. \n",
      "Epoch 46. Loss train 0.653029707090667; validation 0.6199244263577587. \n",
      "Epoch 47. Loss train 0.6495303340557551; validation 0.6173982649048183. \n",
      "Epoch 48. Loss train 0.6452602216891988; validation 0.614309726593369. \n",
      "Epoch 49. Loss train 0.640053248334051; validation 0.6105328661018429. \n",
      "Epoch 50. Loss train 0.6337235616116451; validation 0.6059249078782971. \n",
      "Epoch 51. Loss train 0.6260790254916508; validation 0.6003356884061115. \n",
      "Epoch 52. Loss train 0.6169496805166195; validation 0.5936284232829465. \n",
      "Epoch 53. Loss train 0.6062367324673626; validation 0.5857161305058107. \n",
      "Epoch 54. Loss train 0.593980963421139; validation 0.5766133097418248. \n",
      "Epoch 55. Loss train 0.5804327364248152; validation 0.5664903711092946. \n",
      "Epoch 56. Loss train 0.5660836246803181; validation 0.5557019085306653. \n",
      "Epoch 57. Loss train 0.5516145981545626; validation 0.5447547804754528. \n",
      "Epoch 58. Loss train 0.5377536962415176; validation 0.534207183583175. \n",
      "Epoch 59. Loss train 0.5251019743405353; validation 0.524537514942694. \n",
      "Epoch 60. Loss train 0.5140155391676339; validation 0.5160475714712922. \n",
      "Epoch 61. Loss train 0.5045886864859305; validation 0.5088390333822884. \n",
      "Epoch 62. Loss train 0.4967159624358354; validation 0.5028530465811708. \n",
      "Epoch 63. Loss train 0.4901811231492562; validation 0.49793582915913487. \n",
      "Epoch 64. Loss train 0.4847337191036667; validation 0.49389852262310946. \n",
      "Epoch 65. Loss train 0.4801389945303897; validation 0.49055754300501514. \n",
      "Epoch 66. Loss train 0.4762028972432605; validation 0.4877553861105999. \n",
      "Epoch 67. Loss train 0.4727790789063396; validation 0.48536734210460386. \n",
      "Epoch 68. Loss train 0.4697644661281062; validation 0.483299944372301. \n",
      "Epoch 69. Loss train 0.46708886470394567; validation 0.48148557704306133. \n",
      "Epoch 70. Loss train 0.4647032574162272; validation 0.4798761100395455. \n",
      "Epoch 71. Loss train 0.4625702193505576; validation 0.4784371459314014. \n",
      "Epoch 72. Loss train 0.4606579495812396; validation 0.4771435220032773. \n",
      "Epoch 73. Loss train 0.4589376047276831; validation 0.47597612258387767. \n",
      "Epoch 74. Loss train 0.45738269975858414; validation 0.4749197740467547. \n",
      "Epoch 75. Loss train 0.45596935073441586; validation 0.4739619239023148. \n",
      "Epoch 76. Loss train 0.45467659947004424; validation 0.4730918404630322. \n",
      "Epoch 77. Loss train 0.45348652452278004; validation 0.47230013555711864. \n",
      "Epoch 78. Loss train 0.45238411675023055; validation 0.4715784759581312. \n",
      "Epoch 79. Loss train 0.451356998109863; validation 0.47091939955141016. \n",
      "Epoch 80. Loss train 0.45039506928913653; validation 0.47031618808737824. \n",
      "Epoch 81. Loss train 0.44949014702825735; validation 0.4697627712486696. \n",
      "Epoch 82. Loss train 0.4486356254931319; validation 0.4692536498639162. \n",
      "Epoch 83. Loss train 0.4478261769676698; validation 0.46878383282442815. \n",
      "Epoch 84. Loss train 0.4470574957616467; validation 0.46834878528639834. \n",
      "Epoch 85. Loss train 0.4463260834084803; validation 0.4679443868346398. \n",
      "Epoch 86. Loss train 0.44562907080399783; validation 0.46756689846364413. \n",
      "Epoch 87. Loss train 0.44496407237530305; validation 0.4672129370381541. \n",
      "Epoch 88. Loss train 0.4443290677009713; validation 0.4668794556045343. \n",
      "Epoch 89. Loss train 0.4437223066572888; validation 0.4665637276848258. \n",
      "Epoch 90. Loss train 0.4431422348192173; validation 0.46626333359004. \n",
      "Epoch 91. Loss train 0.44258743634364833; validation 0.46597614689748584. \n",
      "Epoch 92. Loss train 0.4420565918660033; validation 0.46570031956984415. \n",
      "Epoch 93. Loss train 0.44154844908693414; validation 0.46543426472083416. \n",
      "Epoch 94. Loss train 0.44106180379170684; validation 0.465176636669163. \n",
      "Epoch 95. Loss train 0.4405954891121416; validation 0.4649263085491926. \n",
      "Epoch 96. Loss train 0.4401483709663694; validation 0.46468234824406407. \n",
      "Epoch 97. Loss train 0.43971934781523425; validation 0.46444399369351624. \n",
      "Epoch 98. Loss train 0.43930735314388586; validation 0.46421062868276797. \n",
      "Epoch 99. Loss train 0.43891135938271464; validation 0.4639817600769115. \n",
      "Epoch 100. Loss train 0.43853038228976976; validation 0.46375699720006053. \n",
      "Epoch 101. Loss train 0.4381634851004129; validation 0.4635360337508913. \n",
      "Epoch 102. Loss train 0.4378097819930354; validation 0.4633186323623324. \n",
      "Epoch 103. Loss train 0.4374684406158823; validation 0.46310461169297495. \n",
      "Epoch 104. Loss train 0.4371386835700587; validation 0.4628938357951712. \n",
      "Epoch 105. Loss train 0.43681978885232836; validation 0.46268620543431616. \n",
      "Epoch 106. Loss train 0.4365110893348018; validation 0.4624816510195517. \n",
      "Epoch 107. Loss train 0.43621197140382156; validation 0.4622801268292123. \n",
      "Epoch 108. Loss train 0.43592187290367285; validation 0.46208160625769. \n",
      "Epoch 109. Loss train 0.43564028053781895; validation 0.4618860778610509. \n",
      "Epoch 110. Loss train 0.43536672687597594; validation 0.4616935420281122. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111. Loss train 0.43510078710341926; validation 0.46150400814703246. \n",
      "Epoch 112. Loss train 0.4348420756325598; validation 0.46131749217278073. \n",
      "Epoch 113. Loss train 0.4345902426784035; validation 0.4611340145279639. \n",
      "Epoch 114. Loss train 0.43434497088072965; validation 0.4609535982892516. \n",
      "Epoch 115. Loss train 0.43410597203786017; validation 0.4607762676252868. \n",
      "Epoch 116. Loss train 0.43387298400049407; validation 0.4606020464609294. \n",
      "Epoch 117. Loss train 0.4336457677596488; validation 0.46043095734816636. \n",
      "Epoch 118. Loss train 0.4334241047504445; validation 0.46026302052713336. \n",
      "Epoch 119. Loss train 0.43320779438327905; validation 0.46009825316225206. \n",
      "Epoch 120. Loss train 0.43299665180574165; validation 0.459936668739125. \n",
      "Epoch 121. Loss train 0.43279050589222395; validation 0.459778276608007. \n",
      "Epoch 122. Loss train 0.4325891974533712; validation 0.45962308165966215. \n",
      "Epoch 123. Loss train 0.4323925776540552; validation 0.4594710841194398. \n",
      "Epoch 124. Loss train 0.43220050662621984; validation 0.4593222794455346. \n",
      "Epoch 125. Loss train 0.4320128522615355; validation 0.4591766583177055. \n",
      "Epoch 126. Loss train 0.4318294891681224; validation 0.4590342067032168. \n",
      "Epoch 127. Loss train 0.4316502977755012; validation 0.4588949059874204. \n",
      "Epoch 128. Loss train 0.4314751635722572; validation 0.4587587331571848. \n",
      "Epoch 129. Loss train 0.4313039764615572; validation 0.45862566102628205. \n",
      "Epoch 130. Loss train 0.4311366302205243; validation 0.45849565849280866. \n",
      "Epoch 131. Loss train 0.430973022050491; validation 0.4583686908197225. \n",
      "Epoch 132. Loss train 0.4308130522062406; validation 0.45824471993060384. \n",
      "Epoch 133. Loss train 0.43065662369346785; validation 0.4581237047137393. \n",
      "Epoch 134. Loss train 0.43050364202480007; validation 0.458005601328611. \n",
      "Epoch 135. Loss train 0.43035401502579185; validation 0.45789036350978424. \n",
      "Epoch 136. Loss train 0.4302076526833197; validation 0.45777794286405465. \n",
      "Epoch 137. Loss train 0.43006446702973583; validation 0.4576682891575115. \n",
      "Epoch 138. Loss train 0.42992437205699596; validation 0.45756135058989605. \n",
      "Epoch 139. Loss train 0.42978728365573526; validation 0.4574570740542974. \n",
      "Epoch 140. Loss train 0.4296531195749373; validation 0.4573554053807992. \n",
      "Epoch 141. Loss train 0.42952179939842045; validation 0.45725628956321945. \n",
      "Epoch 142. Loss train 0.42939324453485894; validation 0.457159670968521. \n",
      "Epoch 143. Loss train 0.42926737821846805; validation 0.4570654935288658. \n",
      "Epoch 144. Loss train 0.4291441255178244; validation 0.4569737009166028. \n",
      "Epoch 145. Loss train 0.4290234133505638; validation 0.4568842367027583. \n",
      "Epoch 146. Loss train 0.4289051705019252; validation 0.4567970444998063. \n",
      "Epoch 147. Loss train 0.42878932764528466; validation 0.45671206808967324. \n",
      "Epoch 148. Loss train 0.42867581736297505; validation 0.45662925153804984. \n",
      "Epoch 149. Loss train 0.42856457416581195; validation 0.4565485392961668. \n",
      "Epoch 150. Loss train 0.4284555345098663; validation 0.4564698762912286. \n",
      "Epoch 151. Loss train 0.4283486368091478; validation 0.45639320800671385. \n",
      "Epoch 152. Loss train 0.42824382144299095; validation 0.4563184805537114. \n",
      "Epoch 153. Loss train 0.4281410307570824; validation 0.4562456407344146. \n",
      "Epoch 154. Loss train 0.4280402090572388; validation 0.4561746360988108. \n",
      "Epoch 155. Loss train 0.42794130259523416; validation 0.4561054149954997. \n",
      "Epoch 156. Loss train 0.42784425954618477; validation 0.4560379266174594. \n",
      "Epoch 157. Loss train 0.4277490299772324; validation 0.4559721210434477. \n",
      "Epoch 158. Loss train 0.42765556580750697; validation 0.4559079492755979. \n",
      "Epoch 159. Loss train 0.4275638207595917; validation 0.45584536327362674. \n",
      "Epoch 160. Loss train 0.4274737503029553; validation 0.45578431598595887. \n",
      "Epoch 161. Loss train 0.4273853115900373; validation 0.45572476137794077. \n",
      "Epoch 162. Loss train 0.4272984633858715; validation 0.4556666544572273. \n",
      "Epoch 163. Loss train 0.42721316599229714; validation 0.4556099512963326. \n",
      "Epoch 164. Loss train 0.42712938116793364; validation 0.45555460905227485. \n",
      "Epoch 165. Loss train 0.4270470720451744; validation 0.4555005859832004. \n",
      "Epoch 166. Loss train 0.42696620304549243; validation 0.45544784146184686. \n",
      "Epoch 167. Loss train 0.42688673979433567; validation 0.4553963359857018. \n",
      "Epoch 168. Loss train 0.4268086490368431; validation 0.4553460311837269. \n",
      "Epoch 169. Loss train 0.42673189855551547; validation 0.45529688981954103. \n",
      "Epoch 170. Loss train 0.4266564570908566; validation 0.45524887579099665. \n",
      "Epoch 171. Loss train 0.4265822942658556; validation 0.4552019541261289. \n",
      "Epoch 172. Loss train 0.42650938051501835; validation 0.4551560909755087. \n",
      "Epoch 173. Loss train 0.4264376870184915; validation 0.4551112536010786. \n",
      "Epoch 174. Loss train 0.4263671856416521; validation 0.45506741036160675. \n",
      "Epoch 175. Loss train 0.4262978488803773; validation 0.4550245306949364. \n",
      "Epoch 176. Loss train 0.4262296498120589; validation 0.45498258509724976. \n",
      "Epoch 177. Loss train 0.42616256205229486; validation 0.454941545099604. \n",
      "Epoch 178. Loss train 0.42609655971707583; validation 0.45490138324201695. \n",
      "Epoch 179. Loss train 0.4260316173901931; validation 0.45486207304540294. \n",
      "Epoch 180. Loss train 0.4259677100955177; validation 0.45482358898167147. \n",
      "Epoch 181. Loss train 0.42590481327375274; validation 0.45478590644230105. \n",
      "Epoch 182. Loss train 0.42584290276322223; validation 0.45474900170570215. \n",
      "Epoch 183. Loss train 0.4257819547842446; validation 0.4547128519036698. \n",
      "Epoch 184. Loss train 0.4257219459266378; validation 0.45467743498722. \n",
      "Epoch 185. Loss train 0.4256628531399086; validation 0.45464272969207764. \n",
      "Epoch 186. Loss train 0.42560465372570166; validation 0.45460871550407317. \n",
      "Epoch 187. Loss train 0.4255473253321085; validation 0.454575372624674. \n",
      "Epoch 188. Loss train 0.42549084594946884; validation 0.454542681936861. \n",
      "Epoch 189. Loss train 0.42543519390733286; validation 0.45451062497153. \n",
      "Epoch 190. Loss train 0.42538034787228807; validation 0.45447918387457814. \n",
      "Epoch 191. Loss train 0.4253262868463936; validation 0.45444834137481005. \n",
      "Epoch 192. Loss train 0.4252729901660008; validation 0.4544180807527773. \n",
      "Epoch 193. Loss train 0.42522043750077154; validation 0.45438838581063956. \n",
      "Epoch 194. Loss train 0.42516860885274294; validation 0.45435924084312274. \n",
      "Epoch 195. Loss train 0.42511748455531045; validation 0.4543306306096243. \n",
      "Epoch 196. Loss train 0.4250670452720352; validation 0.4543025403075027. \n",
      "Epoch 197. Loss train 0.42501727199520006; validation 0.4542749555465751. \n",
      "Epoch 198. Loss train 0.424968146044062; validation 0.45424786232482683. \n",
      "Epoch 199. Loss train 0.42491964906276647; validation 0.4542212470053355. \n",
      "Epoch 200. Loss train 0.42487176301790397; validation 0.4541950962943914. \n",
      "Epoch 201. Loss train 0.424824470195702; validation 0.45416939722079597. \n",
      "Epoch 202. Loss train 0.4247777531988558; validation 0.4541441371163077. \n",
      "Epoch 203. Loss train 0.42473159494301; validation 0.4541193035972033. \n",
      "Epoch 204. Loss train 0.42468597865290864; validation 0.45409488454691227. \n",
      "Epoch 205. Loss train 0.4246408878582377; validation 0.45407086809968417. \n",
      "Epoch 206. Loss train 0.4245963063891861; validation 0.4540472426252433. \n",
      "Epoch 207. Loss train 0.4245522183717528; validation 0.45402399671438254. \n",
      "Epoch 208. Loss train 0.42450860822283276; validation 0.4540011191654494. \n",
      "Epoch 209. Loss train 0.42446546064510987; validation 0.45397859897167464. \n",
      "Epoch 210. Loss train 0.4244227606217879; validation 0.4539564253092972. \n",
      "Epoch 211. Loss train 0.42438049341119133; validation 0.45393458752643384. \n",
      "Epoch 212. Loss train 0.4243386445412613; validation 0.45391307513265133. \n",
      "Epoch 213. Loss train 0.4242971998039774; validation 0.45389187778919104. \n",
      "Epoch 214. Loss train 0.4242561452497293; validation 0.45387098529980624. \n",
      "Epoch 215. Loss train 0.42421546718166353; validation 0.45385038760216473. \n",
      "Epoch 216. Loss train 0.4241751521500264; validation 0.4538300747597823. \n",
      "Epoch 217. Loss train 0.42413518694652563; validation 0.45381003695444394. \n",
      "Epoch 218. Loss train 0.424095558598728; validation 0.45379026447907783. \n",
      "Epoch 219. Loss train 0.4240562543645107; validation 0.45377074773104953. \n",
      "Epoch 220. Loss train 0.42401726172658133; validation 0.45375147720584164. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221. Loss train 0.4239785683870804; validation 0.4537324434910908. \n",
      "Epoch 222. Loss train 0.4239401622622791; validation 0.4537136372609536. \n",
      "Epoch 223. Loss train 0.4239020314773809; validation 0.4536950492707749. \n",
      "Epoch 224. Loss train 0.4238641643614388; validation 0.4536766703520364. \n",
      "Epoch 225. Loss train 0.42382654944239395; validation 0.4536584914075611. \n",
      "Epoch 226. Loss train 0.42378917544224354; validation 0.45364050340695566. \n",
      "Epoch 227. Loss train 0.4237520312723434; validation 0.45362269738227085. \n",
      "Epoch 228. Loss train 0.4237151060288494; validation 0.45360506442386533. \n",
      "Epoch 229. Loss train 0.42367838898830157; validation 0.45358759567645507. \n",
      "Epoch 230. Loss train 0.4236418696033541; validation 0.45357028233533736. \n",
      "Epoch 231. Loss train 0.42360553749865326; validation 0.45355311564277556. \n",
      "Epoch 232. Loss train 0.4235693824668633; validation 0.453536086884536. \n",
      "Epoch 233. Loss train 0.42353339446484345; validation 0.4535191873865644. \n",
      "Epoch 234. Loss train 0.4234975636099739; validation 0.45350240851179796. \n",
      "Epoch 235. Loss train 0.42346188017663206; validation 0.4534857416571014. \n",
      "Epoch 236. Loss train 0.42342633459281726; validation 0.4534691782503246. \n",
      "Epoch 237. Loss train 0.42339091743692375; validation 0.4534527097474755. \n",
      "Epoch 238. Loss train 0.4233556194346595; validation 0.45343632763000297. \n",
      "Epoch 239. Loss train 0.42332043145610965; validation 0.4534200234021891. \n",
      "Epoch 240. Loss train 0.42328534451294264; validation 0.4534037885886466. \n",
      "Epoch 241. Loss train 0.42325034975575543; validation 0.4533876147319196. \n",
      "Epoch 242. Loss train 0.4232154384715575; validation 0.4533714933901887. \n",
      "Epoch 243. Loss train 0.42318060208138836; validation 0.45335541613507785. \n",
      "Epoch 244. Loss train 0.42314583213806756; validation 0.45333937454956547. \n",
      "Epoch 245. Loss train 0.4231111203240734; validation 0.4533233602259991. \n",
      "Epoch 246. Loss train 0.4230764584495474; validation 0.45330736476421657. \n",
      "Epoch 247. Loss train 0.4230418384504199; validation 0.45329137976977424. \n",
      "Epoch 248. Loss train 0.423007252386657; validation 0.45327539685228574. \n",
      "Epoch 249. Loss train 0.4229726924406201; validation 0.4532594076238748. \n",
      "Epoch 250. Loss train 0.4229381509155398; validation 0.45324340369774363. \n",
      "Epoch 251. Loss train 0.4229036202340974; validation 0.45322737668686186. \n",
      "Epoch 252. Loss train 0.4228690929371105; validation 0.4532113182027804. \n",
      "Epoch 253. Loss train 0.4228345616823212; validation 0.45319521985457345. \n",
      "Epoch 254. Loss train 0.42280001924328087; validation 0.45317907324791407. \n",
      "Epoch 255. Loss train 0.4227654585083296; validation 0.4531628699842881. \n",
      "Epoch 256. Loss train 0.4227308724796652; validation 0.45314660166035214. \n",
      "Epoch 257. Loss train 0.42269625427249946; validation 0.4531302598674396. \n",
      "Epoch 258. Loss train 0.42266159711429757; validation 0.45311383619122164. \n",
      "Epoch 259. Loss train 0.4226268943440961; validation 0.45309732221152826. \n",
      "Epoch 260. Loss train 0.42259213941189844; validation 0.45308070950233587. \n",
      "Epoch 261. Loss train 0.4225573258781411; validation 0.45306398963192535. \n",
      "Epoch 262. Loss train 0.4225224474132297; validation 0.45304715416322044. \n",
      "Epoch 263. Loss train 0.42248749779714107; validation 0.45303019465430694. \n",
      "Epoch 264. Loss train 0.4224524709190861; validation 0.453013102659145. \n",
      "Epoch 265. Loss train 0.42241736077723335; validation 0.45299586972847455. \n",
      "Epoch 266. Loss train 0.42238216147848656; validation 0.4529784874109241. \n",
      "Epoch 267. Loss train 0.4223468672383162; validation 0.45296094725432645. \n",
      "Epoch 268. Loss train 0.42231147238064004; validation 0.45294324080724696. \n",
      "Epoch 269. Loss train 0.4222759713377503; validation 0.45292535962073077. \n",
      "Epoch 270. Loss train 0.422240358650285; validation 0.4529072952502728. \n",
      "Epoch 271. Loss train 0.42220462896724; validation 0.4528890392580153. \n",
      "Epoch 272. Loss train 0.4221687770460195; validation 0.4528705832151791. \n",
      "Epoch 273. Loss train 0.42213279775252294; validation 0.45285191870472963. \n",
      "Epoch 274. Loss train 0.42209668606126444; validation 0.45283303732428365. \n",
      "Epoch 275. Loss train 0.42206043705552415; validation 0.4528139306892575. \n",
      "Epoch 276. Loss train 0.42202404592752807; validation 0.4527945904362604. \n",
      "Epoch 277. Loss train 0.4219875079786545; validation 0.45277500822673455. \n",
      "Epoch 278. Loss train 0.42195081861966527; validation 0.4527551757508407. \n",
      "Epoch 279. Loss train 0.4219139733709595; validation 0.45273508473159163. \n",
      "Epoch 280. Loss train 0.42187696786284834; validation 0.45271472692923176. \n",
      "Epoch 281. Loss train 0.4218397978358478; validation 0.45269409414585987. \n",
      "Epoch 282. Loss train 0.4218024591409901; validation 0.4526731782302923. \n",
      "Epoch 283. Loss train 0.42176494774014894; validation 0.452651971083164. \n",
      "Epoch 284. Loss train 0.42172725970637875; validation 0.45263046466226126. \n",
      "Epoch 285. Loss train 0.4216893912242669; validation 0.4526086509880768. \n",
      "Epoch 286. Loss train 0.4216513385902943; validation 0.4525865221495851. \n",
      "Epoch 287. Loss train 0.42161309821320564; validation 0.45256407031022194. \n",
      "Epoch 288. Loss train 0.42157466661438625; validation 0.4525412877140636. \n",
      "Epoch 289. Loss train 0.42153604042824244; validation 0.4525181666921883. \n",
      "Epoch 290. Loss train 0.42149721640258636; validation 0.4524946996692086. \n",
      "Epoch 291. Loss train 0.42145819139901985; validation 0.45247087916995943. \n",
      "Epoch 292. Loss train 0.42141896239331744; validation 0.4524466978263228. \n",
      "Epoch 293. Loss train 0.4213795264758054; validation 0.45242214838417283. \n",
      "Epoch 294. Loss train 0.4213398808517336; validation 0.45239722371041696. \n",
      "Epoch 295. Loss train 0.4213000228416378; validation 0.45237191680011524. \n",
      "Epoch 296. Loss train 0.42125994988169; validation 0.45234622078365067. \n",
      "Epoch 297. Loss train 0.4212196595240314; validation 0.45232012893392637. \n",
      "Epoch 298. Loss train 0.4211791494370869; validation 0.4522936346735613. \n",
      "Epoch 299. Loss train 0.4211384174058548; validation 0.4522667315820577. \n",
      "Epoch 300. Loss train 0.42109746133216874; validation 0.4522394134029067. \n",
      "Epoch 301. Loss train 0.42105627923492694; validation 0.4522116740506048. \n",
      "Epoch 302. Loss train 0.4210148692502826; validation 0.4521835076175426. \n",
      "Epoch 303. Loss train 0.42097322963179157; validation 0.45215490838073763. \n",
      "Epoch 304. Loss train 0.42093135875051063; validation 0.4521258708083709. \n",
      "Epoch 305. Loss train 0.42088925509503927; validation 0.4520963895660943. \n",
      "Epoch 306. Loss train 0.42084691727150025; validation 0.45206645952307156. \n",
      "Epoch 307. Loss train 0.4208043440034501; validation 0.452036075757714. \n",
      "Epoch 308. Loss train 0.4207615341317133; validation 0.45200523356307615. \n",
      "Epoch 309. Loss train 0.42071848661413197; validation 0.4519739284518733. \n",
      "Epoch 310. Loss train 0.4206752005252232; validation 0.45194215616108213. \n",
      "Epoch 311. Loss train 0.42063167505573595; validation 0.4519099126560912. \n",
      "Epoch 312. Loss train 0.4205879095120987; validation 0.451877194134363. \n",
      "Epoch 313. Loss train 0.42054390331574937; validation 0.45184399702857636. \n",
      "Epoch 314. Loss train 0.4204996560023396; validation 0.4518103180092138. \n",
      "Epoch 315. Loss train 0.4204551672208027; validation 0.45177615398656473. \n",
      "Epoch 316. Loss train 0.4204104367322793; validation 0.45174150211211545. \n",
      "Epoch 317. Loss train 0.4203654644088898; validation 0.4517063597793001. \n",
      "Epoch 318. Loss train 0.4203202502323465; validation 0.45167072462358926. \n",
      "Epoch 319. Loss train 0.4202747942923978; validation 0.45163459452189536. \n",
      "Epoch 320. Loss train 0.42022909678509507; validation 0.45159796759128074. \n",
      "Epoch 321. Loss train 0.42018315801087774; validation 0.45156084218695214. \n",
      "Epoch 322. Loss train 0.4201369783724684; validation 0.4515232168995348. \n",
      "Epoch 323. Loss train 0.42009055837257225; validation 0.45148509055162206. \n",
      "Epoch 324. Loss train 0.42004389861137714; validation 0.45144646219359735. \n",
      "Epoch 325. Loss train 0.4199969997838495; validation 0.4514073310987359. \n",
      "Epoch 326. Loss train 0.41994986267682377; validation 0.45136769675759303. \n",
      "Epoch 327. Loss train 0.41990248816588277; validation 0.45132755887169335. \n",
      "Epoch 328. Loss train 0.41985487721203; validation 0.45128691734653953. \n",
      "Epoch 329. Loss train 0.41980703085815374; validation 0.4512457722839632. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330. Loss train 0.41975895022528514; validation 0.45120412397384657. \n",
      "Epoch 331. Loss train 0.41971063650865525; validation 0.45116197288524656. \n",
      "Epoch 332. Loss train 0.41966209097355495; validation 0.4511193196569595. \n",
      "Epoch 333. Loss train 0.41961331495100607; validation 0.4510761650875666. \n",
      "Epoch 334. Loss train 0.41956430983325094; validation 0.451032510125005. \n",
      "Epoch 335. Loss train 0.4195150770690725; validation 0.4509883558557157. \n",
      "Epoch 336. Loss train 0.41946561815895544; validation 0.4509437034934178. \n",
      "Epoch 337. Loss train 0.4194159346501024; validation 0.4508985543675658. \n",
      "Epoch 338. Loss train 0.419366028131322; validation 0.45085290991154825. \n",
      "Epoch 339. Loss train 0.41931590022780435; validation 0.4508067716506864. \n",
      "Epoch 340. Loss train 0.41926555259580345; validation 0.45076014119009583. \n",
      "Epoch 341. Loss train 0.4192149869172476; validation 0.45071302020247356. \n",
      "Epoch 342. Loss train 0.4191642048942992; validation 0.4506654104158739. \n",
      "Epoch 343. Loss train 0.41911320824388776; validation 0.4506173136015379. \n",
      "Epoch 344. Loss train 0.419061998692242; validation 0.45056873156183963. \n",
      "Epoch 345. Loss train 0.41901057796944646; validation 0.450519666118411. \n",
      "Epoch 346. Loss train 0.4189589478040514; validation 0.4504701191005098. \n",
      "Epoch 347. Loss train 0.4189071099177654; validation 0.45042009233368635. \n",
      "Epoch 348. Loss train 0.4188550660202593; validation 0.4503695876288114. \n",
      "Epoch 349. Loss train 0.4188028178041147; validation 0.4503186067715174. \n",
      "Epoch 350. Loss train 0.4187503669399488; validation 0.4502671515121049. \n",
      "Epoch 351. Loss train 0.41869771507174713; validation 0.45021522355596466. \n",
      "Epoch 352. Loss train 0.41864486381243987; validation 0.4501628245545608. \n",
      "Epoch 353. Loss train 0.41859181473975415; validation 0.45010995609701215. \n",
      "Epoch 354. Loss train 0.41853856939237793; validation 0.45005661970231353. \n",
      "Epoch 355. Loss train 0.4184851292664692; validation 0.4500028168122243. \n",
      "Epoch 356. Loss train 0.41843149581254496; validation 0.4499485487848557. \n",
      "Epoch 357. Loss train 0.4183776704327845; validation 0.44989381688897506. \n",
      "Epoch 358. Loss train 0.4183236544787792; validation 0.44983862229904736. \n",
      "Epoch 359. Loss train 0.41826944924976256; validation 0.4497829660910237. \n",
      "Epoch 360. Loss train 0.41821505599135017; validation 0.4497268492388848. \n",
      "Epoch 361. Loss train 0.4181604758948198; validation 0.4496702726119391. \n",
      "Epoch 362. Loss train 0.41810571009695807; validation 0.44961323697287287. \n",
      "Epoch 363. Loss train 0.4180507596804991; validation 0.44955574297654105. \n",
      "Epoch 364. Loss train 0.4179956256751738; validation 0.4494977911694853. \n",
      "Epoch 365. Loss train 0.41794030905939145; validation 0.44943938199015593. \n",
      "Epoch 366. Loss train 0.41788481076256123; validation 0.44938051576981736. \n",
      "Epoch 367. Loss train 0.4178291316680664; validation 0.44932119273409915. \n",
      "Epoch 368. Loss train 0.4177732726168888; validation 0.44926141300516403. \n",
      "Epoch 369. Loss train 0.4177172344118822; validation 0.44920117660444703. \n",
      "Epoch 370. Loss train 0.41766101782268045; validation 0.4491404834559249. \n",
      "Epoch 371. Loss train 0.41760462359122114; validation 0.4490793333898635. \n",
      "Epoch 372. Loss train 0.4175480524378564; validation 0.4490177261469931. \n",
      "Epoch 373. Loss train 0.4174913050680118; validation 0.4489556613830545. \n",
      "Epoch 374. Loss train 0.4174343821793465; validation 0.44889313867365843. \n",
      "Epoch 375. Loss train 0.41737728446935596; validation 0.4488301575193951. \n",
      "Epoch 376. Loss train 0.4173200126433495; validation 0.44876671735113516. \n",
      "Epoch 377. Loss train 0.417262567422723; validation 0.4487028175354578. \n",
      "Epoch 378. Loss train 0.4172049495534394; validation 0.4486384573801434. \n",
      "Epoch 379. Loss train 0.4171471598146177; validation 0.4485736361396713. \n",
      "Epoch 380. Loss train 0.41708919902712377; validation 0.44850835302066255. \n",
      "Epoch 381. Loss train 0.417031068062049; validation 0.4484426071872143. \n",
      "Epoch 382. Loss train 0.41697276784895676; validation 0.4483763977660737. \n",
      "Epoch 383. Loss train 0.4169142993837725; validation 0.44830972385160806. \n",
      "Epoch 384. Loss train 0.4168556637361908; validation 0.4482425845105307. \n",
      "Epoch 385. Loss train 0.41679686205647726; validation 0.4481749787863564. \n",
      "Epoch 386. Loss train 0.4167378955815399; validation 0.44810690570356. \n",
      "Epoch 387. Loss train 0.41667876564015993; validation 0.4480383642714299. \n",
      "Epoch 388. Loss train 0.41661947365727325; validation 0.4479693534876116. \n",
      "Epoch 389. Loss train 0.416560021157213; validation 0.44789987234135087. \n",
      "Epoch 390. Loss train 0.41650040976583497; validation 0.44782991981645537. \n",
      "Epoch 391. Loss train 0.4164406412114692; validation 0.44775949489400013. \n",
      "Epoch 392. Loss train 0.4163807173246586; validation 0.44768859655481646. \n",
      "Epoch 393. Loss train 0.4163206400366709; validation 0.44761722378181124. \n",
      "Epoch 394. Loss train 0.41626041137679143; validation 0.44754537556216367. \n",
      "Epoch 395. Loss train 0.41620003346843015; validation 0.4474730508894642. \n",
      "Epoch 396. Loss train 0.416139508524099; validation 0.4474002487658494. \n",
      "Epoch 397. Loss train 0.41607883883934027; validation 0.44732696820419965. \n",
      "Epoch 398. Loss train 0.41601802678570493; validation 0.44725320823045717. \n",
      "Epoch 399. Loss train 0.41595707480290345; validation 0.4471789678861221. \n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(xtr,ytr,batchSize=16,epochs=400,learningRate=0.4,lmbda=0.01,dropout=0.1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting for the test set, computing accuracy, recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8100558659217877.\n",
      "Precision: 0.8653846153846154, Recall: 0.625.\n"
     ]
    }
   ],
   "source": [
    "pred = nn.predict(xts).T                                     #prediction for the test set                               \n",
    "acc = sum((pred.reshape(yts.shape) > 0.5) == yts)/len(yts)   #accuracy\n",
    "\n",
    "truePos, falsePos, pos = 0,0,0\n",
    "for predVal,actVal in zip(pred.reshape(yts.shape) > 0.5,yts):\n",
    "    truePos += (actVal == 1 and predVal == 1)\n",
    "    falsePos += (actVal == 0 and predVal == 1)\n",
    "    pos += actVal\n",
    "precision = truePos / (truePos + falsePos)                   #precision\n",
    "recall = truePos / pos                                       #recall\n",
    "\n",
    "print(\"Accuracy: {}.\".format(acc))\n",
    "print(\"Precision: {}, Recall: {}.\".format(precision,recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview of the neural network learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainLoss = [epoch['trainLoss'] for epoch in history]\n",
    "valLoss = [epoch['validationLoss'] for epoch in history]\n",
    "\n",
    "plt.plot([i for i in range(len(trainLoss))],trainLoss,c = 'b',label='train')\n",
    "plt.plot([i for i in range(len(valLoss))], valLoss,c = 'r',label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Neural network learning curve\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
